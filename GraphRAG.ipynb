{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. Conda Environment\n",
        "\n",
        "\n",
        "    Please create a Python 3.10-3.12 Conda environment with below commands. \n",
        "\n",
        "        conda create -n graph python=3.11 -y && conda activate graph\n",
        "\n",
        "        pip install graphrag\n",
        "\n",
        "        conda activate graph\n",
        "\n",
        "    We will use the \"graph\" Conda environment to run the below notebook and create the graph indexes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. Input Data\n",
        "\n",
        "    The text file need to be placed inside an **input** folder so that the Graph init command can read the text files\n",
        "\n",
        "        - The file location for this notebook is a local drive\n",
        "\n",
        "    You can also use the Microsoft Fabric OneLake location by following below steps \n",
        "\n",
        "        - Download and Install the Onelake file explorere from https://www.microsoft.com/en-us/download/details.aspx?id=105222 \n",
        "\n",
        "        - Once installed you will be able to browse to your Fabric workspace and oneLake\n",
        "\n",
        "        - Copy the text file which you want to be gaphed inside the Files folder in oneLake"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GraphRag Initialization\n",
        "\n",
        "Use init Command to initialize the Grapgh , in our case the text files are inside **/input** folder\n",
        "\n",
        "init command will craete .env and settings.yaml files and a prompts folder with the default prompts to extract the graph entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# assumng that the current directory is the root of the project having input folder\n",
        "!python -m graphrag.index --init --root ./\n",
        "#!python -m graphrag.index --init --root \"C:\\Users\\safdarzaman\\OneLake - Microsoft\\My workspace\\yellowtaxidatalakehouse.Lakehouse\\Files\\Ragdata\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Prior to running the indexing job in next cell, please update the llm, embeddings and snapshots sections in settings.yaml\n",
        "\n",
        "        llm:\n",
        "            api_key: <api key for Azure Open AI model>\n",
        "            type: azure_openai_chat\n",
        "            model: gpt-4o\n",
        "            model_supports_json: true # recommended if this is available for your model.\n",
        "            max_tokens: 4096\n",
        "            request_timeout: 180.0\n",
        "            api_base: https://xxxx.openai.azure.com\n",
        "            api_version: 2024-02-15-preview\n",
        "            organization: Microsoft\n",
        "            deployment_name: <Name of your deployment>\n",
        "\n",
        "        embeddings:\n",
        "        ## parallelization: override the global parallelization settings for embeddings\n",
        "        async_mode: threaded # or asyncio\n",
        "        # target: required # or all\n",
        "            llm:\n",
        "                api_key: <api key for Azure Open AI model>\n",
        "                type: azure_openai_embedding\n",
        "                model: text-embedding-3-large\n",
        "                api_base: https://xxxx.openai.azure.com\n",
        "                api_version: 2024-02-15-preview\n",
        "                organization: Microsoft\n",
        "                deployment_name: <Name of your deployment>\n",
        "\n",
        "        snapshots:\n",
        "                graphml: true\n",
        "                raw_entities: true\n",
        "                top_level_nodes: true\n",
        "\n",
        "        \n",
        "Update the .env file and provide the azure open ai api key in the file \n",
        "\n",
        "            \n",
        "Note: setting gaphml parameter to true to create the graph for visualization   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Graph Indexing\n",
        "\n",
        "graph.index pipeline will create the  parquet files under the output folder with entities, relatiosnships and community reports "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# assumng that the current directory is the root of the project having input folder\n",
        "!python -m graphrag.index --root ./\n",
        "#uncomemnt the below for blob storage or Microsoft OneLake \n",
        "#!python -m graphrag.index --root \"C:\\Users\\safdarzaman\\OneLake - Microsoft\\My workspace\\yellowtaxidatalakehouse.Lakehouse\\Files\\Ragdata\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "After successful running of the index pipeline you will have multiple parquet and graphml files under artifacts folder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize the Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import glob\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load the GraphML file\n",
        "graph = nx.read_graphml(glob.glob(\"./output/20241002-102507/artifacts/merged_graph.graphml\")[0])\n",
        "\n",
        "fig=plt.figure(figsize=(20, 20))\n",
        "# Extract node types\n",
        "node_types = nx.get_node_attributes(graph, 'type')\n",
        "\n",
        "# Define a color map for the node types\n",
        "unique_types = list(set(node_types.values()))\n",
        "color_map = {node_type: plt.cm.tab20(i / len(unique_types)) for i, node_type in enumerate(unique_types)}\n",
        "\n",
        "# Assign colors to nodes based on their type\n",
        "node_colors = [color_map[node_types[node]] if node in node_types else \"r\" for node in graph.nodes]\n",
        "\n",
        "# Calculate PageRank\n",
        "pagerank = nx.pagerank(graph)\n",
        "\n",
        "# Assign node sizes based on PageRank\n",
        "#node_sizes = [pagerank[node] * 10000 for node in graph.nodes]\n",
        "\n",
        "# Calculate node degree\n",
        "node_degree = dict(graph.degree())\n",
        "\n",
        "# Assign node sizes based on node degree\n",
        "node_sizes = [node_degree[node]*60 for node in graph.nodes]\n",
        "\n",
        "# Assign font sizes based on node degree\n",
        "font_sizes = {node:np.sqrt(node_degree[node]) * 0.15 for node in graph.nodes}\n",
        "\n",
        "pos = nx.spring_layout(graph, k=0.20, iterations=20)\n",
        "#nx.draw(graph, pos, with_labels=True, font_size=font_sizes, font_color='black', font_weight='bold', node_color=node_colors, node_size=node_sizes, edge_color='gray', linewidths=0.5, alpha=0.9)\n",
        "nx.draw(graph, pos, with_labels=False, node_color=node_colors, node_size=node_sizes, edge_color='gray', linewidths=0.5, alpha=0.9)\n",
        "for node, (x, y) in pos.items():\n",
        "    plt.text(x, y, node, fontsize=font_sizes[node]*5, ha='center', va='center')\n",
        "fig.set_facecolor(\"black\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run queries on the created Graph entities "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install graphrag\n",
        "%pip install azure-core\n",
        "%pip install langchain\n",
        "%pip install tiktoken\n",
        "%pip install langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_openai import AzureOpenAIEmbeddings\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "\n",
        "from graphrag.query.indexer_adapters import read_indexer_entities, read_indexer_reports\n",
        "from graphrag.query.llm.oai.chat_openai import ChatOpenAI\n",
        "from graphrag.query.llm.oai.embedding import OpenAIEmbedding\n",
        "\n",
        "from graphrag.query.llm.oai.typing import OpenaiApiType\n",
        "\n",
        "from graphrag.query.structured_search.global_search.community_context import (\n",
        "    GlobalCommunityContext,\n",
        ")\n",
        "from graphrag.query.structured_search.global_search.search import GlobalSearch\n",
        "\n",
        "from graphrag.query.structured_search.local_search.mixed_context import (\n",
        "    LocalSearchMixedContext,\n",
        ")\n",
        "from graphrag.query.structured_search.local_search.search import LocalSearch\n",
        "from graphrag.vector_stores.lancedb import LanceDBVectorStore\n",
        "\n",
        "from graphrag.query.context_builder.entity_extraction import EntityVectorStoreKey\n",
        "from graphrag.query.indexer_adapters import (\n",
        "    read_indexer_covariates,\n",
        "    read_indexer_entities,\n",
        "    read_indexer_relationships,\n",
        "    read_indexer_reports,\n",
        "    read_indexer_text_units,\n",
        ")\n",
        "\n",
        "\n",
        "from graphrag.query.input.loaders.dfs import (\n",
        "    store_entity_semantic_embeddings,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Provide the LLM configuration which will be used by Graphrag at query time "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {},
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(\n",
        "    api_key=\"XXXXXXXXX\",\n",
        "    model=\"gpt4o\",\n",
        "    api_type=OpenaiApiType.AzureOpenAI,\n",
        "    max_retries=20,\n",
        "    api_base=\"https://XXXXXXXr.openai.azure.com\",\n",
        "    api_version=\"2024-02-15-preview\",\n",
        ")\n",
        "\n",
        "text_embedder = OpenAIEmbedding(\n",
        "    api_key=\"XXXXXX\",\n",
        "    api_base=\"https://XXXXXXX.openai.azure.com\",\n",
        "    api_type=OpenaiApiType.AzureOpenAI,\n",
        "    model=\"text-embedding-3-large\",\n",
        "    deployment_name=\"text-embedding-3-large\",\n",
        "    max_retries=20,\n",
        "    api_version=\"2024-02-15-preview\",\n",
        ")\n",
        "\n",
        "token_encoder = tiktoken.get_encoding(\"cl100k_base\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Provide the path of artifacts folder which has the Parquet files \n",
        "Load the files in Pandas data frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {},
      "outputs": [],
      "source": [
        "#INPUT_DIR = \"C:/Users/safdarzaman/OneLake - Microsoft/My workspace/yellowtaxidatalakehouse.Lakehouse/Files/Ragdata/output/20240925-113329/artifacts\"\n",
        "\n",
        "INPUT_DIR = \"./output/20241002-102507/artifacts\"\n",
        "LANCEDB_URI = f\"{INPUT_DIR}/lancedb\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# parquet files generated from indexing pipeline\n",
        "COMMUNITY_REPORT_TABLE = \"create_final_community_reports\"\n",
        "ENTITY_TABLE = \"create_final_nodes\"\n",
        "ENTITY_EMBEDDING_TABLE = \"create_final_entities\"\n",
        "RELATIONSHIP_TABLE = \"create_final_relationships\"\n",
        "COVARIATE_TABLE = \"create_final_covariates\"\n",
        "TEXT_UNIT_TABLE = \"create_final_text_units\"\n",
        "COMMUNITY_TABLE = \"create_final_communities\"\n",
        "\n",
        "# community level in the Leiden community hierarchy from which we will load the community reports\n",
        "# higher value means we use reports from more fine-grained communities (at the cost of higher computation cost)\n",
        "COMMUNITY_LEVEL = 1\n",
        "\n",
        "\n",
        "\n",
        "# read nodes table to get community and degree data\n",
        "\n",
        "entity_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_TABLE}.parquet\")\n",
        "report_df = pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_REPORT_TABLE}.parquet\")\n",
        "relationship_df = pd.read_parquet(f\"{INPUT_DIR}/{RELATIONSHIP_TABLE}.parquet\")\n",
        "text_unit_df = pd.read_parquet(f\"{INPUT_DIR}/{TEXT_UNIT_TABLE}.parquet\")\n",
        "entity_embedding_df = pd.read_parquet(f\"{INPUT_DIR}/{ENTITY_EMBEDDING_TABLE}.parquet\")\n",
        "community_df= pd.read_parquet(f\"{INPUT_DIR}/{COMMUNITY_TABLE}.parquet\")\n",
        "\n",
        "\n",
        "reports = read_indexer_reports(report_df, entity_df, COMMUNITY_LEVEL)\n",
        "entities = read_indexer_entities(entity_df, entity_embedding_df, COMMUNITY_LEVEL)\n",
        "relationships = read_indexer_relationships(relationship_df)\n",
        "text_units = read_indexer_text_units(text_unit_df)\n",
        "\n",
        "# load description embeddings to an in-memory lancedb vectorstore\n",
        "# to connect to a remote db, specify url and port values.\n",
        "description_embedding_store = LanceDBVectorStore(\n",
        "    collection_name=\"entity_description_embeddings\",\n",
        ")\n",
        "description_embedding_store.connect(db_uri=LANCEDB_URI)\n",
        "entity_description_embeddings = store_entity_semantic_embeddings(\n",
        "    entities=entities, vectorstore=description_embedding_store\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Print the count of Graph entities, relationships, text units and reports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Entity count: {len(entity_df)}\")\n",
        "print(f\"Relationship count: {len(relationship_df)}\")\n",
        "\n",
        "print(f\"Text unit records: {len(text_unit_df)}\")\n",
        "\n",
        "print(f\"Total report count: {len(report_df)}\")\n",
        "\n",
        "print(\n",
        "    f\"Report count after filtering by community level {COMMUNITY_LEVEL}: {len(reports)}\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Let us see the Data Frames which are from our Parquet files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#embedding dataframe\n",
        "entity_embedding_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#relationship dataframe\n",
        "relationship_df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#textunit data frame\n",
        "text_unit_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#report dataframe\n",
        "report_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Run Global Search on data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Global serach is used to answer questions which need the complete dataset understanding for questions like \"What are top 10 themes in articles\"\n",
        "context_builder = GlobalCommunityContext(\n",
        "    community_reports=reports,\n",
        "    entities=entities,  # default to None if you don't want to use community weights for ranking\n",
        "    token_encoder=token_encoder,\n",
        ")\n",
        "\n",
        "\n",
        "context_builder_params = {\n",
        "    \"use_community_summary\": False,  # False means using full community reports. True means using community short summaries.\n",
        "    \"shuffle_data\": True,\n",
        "    \"include_community_rank\": True,\n",
        "    \"min_community_rank\": 0,\n",
        "    \"community_rank_name\": \"rank\",\n",
        "    \"include_community_weight\": True,\n",
        "    \"community_weight_name\": \"occurrence weight\",\n",
        "    \"normalize_community_weight\": True,\n",
        "    \"max_tokens\": 12_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
        "    \"context_name\": \"Reports\",\n",
        "}\n",
        "\n",
        "map_llm_params = {\n",
        "    \"max_tokens\": 1000,\n",
        "    \"temperature\": 0.0,\n",
        "    \"response_format\": {\"type\": \"json_object\"},\n",
        "}\n",
        "\n",
        "reduce_llm_params = {\n",
        "    \"max_tokens\": 2000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000-1500)\n",
        "    \"temperature\": 0.0,\n",
        "}\n",
        "\n",
        "\n",
        "global_search_engine = GlobalSearch(\n",
        "    llm=llm,\n",
        "    context_builder=context_builder,\n",
        "    token_encoder=token_encoder,\n",
        "    max_data_tokens=12_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
        "    map_llm_params=map_llm_params,\n",
        "    reduce_llm_params=reduce_llm_params,\n",
        "    allow_general_knowledge=False,  # set this to True will add instruction to encourage the LLM to incorporate general knowledge in the response, which may increase hallucinations, but could be useful in some use cases.\n",
        "    json_mode=True,  # set this to False if your LLM model does not support JSON mode.\n",
        "    context_builder_params=context_builder_params,\n",
        "    concurrent_coroutines=32,\n",
        "    response_type=\"multiple paragraphs\",  # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#result = await global_search_engine.asearch(\"what is common between all three of states including  ALASKA and Washington, D.C and california, only mention the common thing between them\")\n",
        "#result = await global_search_engine.asearch(\"what is common law in all three of states including  ALASKA and Washington, D.C and california, only mention the common thing between them\")\n",
        "result = await global_search_engine.asearch(\"what is top theme  in the data \")\n",
        "#CONNECTICUT, VERMONT, NEW JERSEY, and PENNYSYLVANIA\n",
        "\n",
        "print(result.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# inspect the data used to build the context for the LLM responses\n",
        "result.context_data[\"reports\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"LLM calls: {result.llm_calls}. LLM tokens: {result.prompt_tokens}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run Local Search on Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Local serach is used to answer questions which need the a very specific and granular part of ou graph for questions like \"what is the name of country with X law\"\n",
        "context_builder = LocalSearchMixedContext(\n",
        "    community_reports=reports,\n",
        "    text_units=text_units,\n",
        "    entities=entities,\n",
        "    relationships=relationships,\n",
        "    # if you did not run covariates during indexing, set this to None\n",
        "    #covariates=covariates,\n",
        "    entity_text_embeddings=description_embedding_store,\n",
        "    embedding_vectorstore_key=EntityVectorStoreKey.ID,  # if the vectorstore uses entity title as ids, set this to EntityVectorStoreKey.TITLE\n",
        "    text_embedder=text_embedder,\n",
        "    token_encoder=token_encoder,\n",
        ")\n",
        "\n",
        "# text_unit_prop: proportion of context window dedicated to related text units\n",
        "# community_prop: proportion of context window dedicated to community reports.\n",
        "# The remaining proportion is dedicated to entities and relationships. Sum of text_unit_prop and community_prop should be <= 1\n",
        "# conversation_history_max_turns: maximum number of turns to include in the conversation history.\n",
        "# conversation_history_user_turns_only: if True, only include user queries in the conversation history.\n",
        "# top_k_mapped_entities: number of related entities to retrieve from the entity description embedding store.\n",
        "# top_k_relationships: control the number of out-of-network relationships to pull into the context window.\n",
        "# include_entity_rank: if True, include the entity rank in the entity table in the context window. Default entity rank = node degree.\n",
        "# include_relationship_weight: if True, include the relationship weight in the context window.\n",
        "# include_community_rank: if True, include the community rank in the context window.\n",
        "# return_candidate_context: if True, return a set of dataframes containing all candidate entity/relationship/covariate records that\n",
        "# could be relevant. Note that not all of these records will be included in the context window. The \"in_context\" column in these\n",
        "# dataframes indicates whether the record is included in the context window.\n",
        "# max_tokens: maximum number of tokens to use for the context window.\n",
        "\n",
        "\n",
        "local_context_params = {\n",
        "    \"text_unit_prop\": 0.5,\n",
        "    \"community_prop\": 0.1,\n",
        "    \"conversation_history_max_turns\": 5,\n",
        "    \"conversation_history_user_turns_only\": True,\n",
        "    \"top_k_mapped_entities\": 10,\n",
        "    \"top_k_relationships\": 10,\n",
        "    \"include_entity_rank\": True,\n",
        "    \"include_relationship_weight\": True,\n",
        "    \"include_community_rank\": False,\n",
        "    \"return_candidate_context\": False,\n",
        "    \"embedding_vectorstore_key\": EntityVectorStoreKey.ID,  # set this to EntityVectorStoreKey.TITLE if the vectorstore uses entity title as ids\n",
        "    \"max_tokens\": 12_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 5000)\n",
        "}\n",
        "\n",
        "llm_params = {\n",
        "    \"max_tokens\": 2_000,  # change this based on the token limit you have on your model (if you are using a model with 8k limit, a good setting could be 1000=1500)\n",
        "    \"temperature\": 0.0,\n",
        "}\n",
        "\n",
        "\n",
        "local_search_engine = LocalSearch(\n",
        "    llm=llm,\n",
        "    context_builder=context_builder,\n",
        "    token_encoder=token_encoder,\n",
        "    llm_params=llm_params,\n",
        "    context_builder_params=local_context_params,\n",
        "    response_type=\"multiple paragraphs\",  # free form text describing the response type and format, can be anything, e.g. prioritized list, single paragraph, multiple paragraphs, multiple-page report\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#result = await local_search_engine.asearch(\"what is common between all three of states including  ALASKA and Washington, D.C and california, only mention the common thing between them\")\n",
        "result = await local_search_engine.asearch(\"what is most important thing in NEW JERSEY transportation networks\")\n",
        "\n",
        "print(result.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result.context_data[\"entities\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result.context_data[\"relationships\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result.context_data[\"reports\"].head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "result.context_data[\"sources\"].head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "grag",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
